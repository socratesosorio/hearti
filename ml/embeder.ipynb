{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/25226366/cropped/\"\n",
    "patient_no = 10\n",
    "data_end = np.array(nib.load(data_path + \"pat\" + str(patient_no) + \"_cropped_seg_endpoints.nii.gz\").get_fdata())\n",
    "data_seg = np.array(nib.load(data_path + \"pat\" + str(patient_no) + \"_cropped_seg.nii.gz\").get_fdata())\n",
    "data_cropped = np.array(nib.load(data_path + \"pat\" + str(patient_no) + \"_cropped.nii.gz\").get_fdata())\n",
    "print(\"endpoint data shape: \" + str(data_end.shape))\n",
    "print(\"cropped data shape: \" + str(data_cropped.shape))\n",
    "print(\"segmented data shape: \" + str(data_seg.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_cropped[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect dataset\n",
    "data_path = \"data/25226366/cropped/\"\n",
    "dataset = []\n",
    "for i in range(60):\n",
    "    data_cropped = torch.tensor(nib.load(data_path + \"pat\" + str(i) + \"_cropped.nii.gz\").get_fdata(), dtype=torch.float32)\n",
    "    data_seg = torch.tensor(nib.load(data_path + \"pat\" + str(i) + \"_cropped_seg.nii.gz\").get_fdata(), dtype=torch.float32)\n",
    "    # split segments into stack\n",
    "    seg_stack = []\n",
    "    for i in range(1,9):\n",
    "        slice = torch.zeros(data_seg.shape)\n",
    "        slice[data_seg == float(i)] = float(i)\n",
    "        seg_stack.append(slice)\n",
    "    dataset.append((data_cropped, torch.stack(seg_stack, dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_cube(tensor):\n",
    "    \"\"\"\n",
    "    Pads a 3D tensor (C, D, H, W) with zeros to make it cubic.\n",
    "    \"\"\"\n",
    "    d, h, w = tensor.shape\n",
    "    max_dim = max(d, h, w)\n",
    "\n",
    "    # Compute padding amounts for each dimension (pad evenly on both sides)\n",
    "    pad_d = (max_dim - d) // 2\n",
    "    pad_h = (max_dim - h) // 2\n",
    "    pad_w = (max_dim - w) // 2\n",
    "\n",
    "    pad = (pad_w, pad_w, pad_h, pad_h, pad_d, pad_d)  # (W, W, H, H, D, D)\n",
    "    padded_tensor = F.pad(tensor, pad, mode='constant', value=0)\n",
    "\n",
    "    return padded_tensor\n",
    "\n",
    "def resize_3d_tensor(tensor, target_size=(128, 128, 128), mode='trilinear'):\n",
    "    \"\"\"\n",
    "    Resizes a 3D tensor to the target size after padding.\n",
    "    \"\"\"\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized_tensor = F.interpolate(tensor, size=target_size, mode=mode, align_corners=False)\n",
    "    return resized_tensor  # Remove batch dimension\n",
    "\n",
    "def preprocess_3d_tensor(tensor, target_size=(128, 128, 128), mode='trilinear'):\n",
    "    \"\"\"\n",
    "    Pads a 3D tensor to make it cubic, then resizes it.\n",
    "    \"\"\"\n",
    "    tensor = pad_to_cube(tensor)\n",
    "    tensor = resize_3d_tensor(tensor, target_size)\n",
    "    return tensor\n",
    "def preprocess_layers(tensor):\n",
    "    out_layers = []\n",
    "    for i in range(tensor.shape[0]):\n",
    "        out_layers.append(preprocess_3d_tensor(tensor[i]))\n",
    "    return torch.stack(out_layers, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_tran_1 = preprocess_3d_tensor(dataset[0][0])\n",
    "print(im_tran_1.shape)\n",
    "plt.imshow(im_tran_1[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            DoubleConv3D(in_channels, 64),\n",
    "            DoubleConv3D(64, 128),\n",
    "            DoubleConv3D(128, 256),\n",
    "            DoubleConv3D(256, 512)\n",
    "        ])\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            DoubleConv3D(512, 1024),\n",
    "            nn.Dropout3d(p=dropout)  # Regularization in the bottleneck\n",
    "        )\n",
    "        \n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose3d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            DoubleConv3D(1024, 512),\n",
    "            DoubleConv3D(512, 256),\n",
    "            DoubleConv3D(256, 128),\n",
    "            DoubleConv3D(128, 64)\n",
    "        ])\n",
    "        \n",
    "        self.final_conv = nn.Conv3d(64, num_classes, kernel_size=1)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.encoder:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(len(self.upconvs)):\n",
    "            x = self.upconvs[idx](x)\n",
    "            skip_connection = skip_connections[idx]\n",
    "            \n",
    "            # Ensure the shapes match before concatenation\n",
    "            if x.shape != skip_connection.shape:\n",
    "                diff_d = skip_connection.shape[2] - x.shape[2]\n",
    "                diff_h = skip_connection.shape[3] - x.shape[3]\n",
    "                diff_w = skip_connection.shape[4] - x.shape[4]\n",
    "                x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
    "                              diff_h // 2, diff_h - diff_h // 2,\n",
    "                              diff_d // 2, diff_d - diff_d // 2])\n",
    "\n",
    "            x = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.decoder[idx](x)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transformed[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 50\n",
    "dataset_transformed = [(preprocess_3d_tensor(t[0]).squeeze(0), preprocess_layers(t[1]).squeeze(1).squeeze(1)) for t in dataset]\n",
    "train_dataloader = DataLoader(dataset_transformed[:split], batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_transformed[split:], batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = UNet3D(in_channels=1, num_classes=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr_0 = 0.0001\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=lr_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_transformed[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # Track loss per epoch\n",
    "    num_batches = 0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[0].to(device)  # Input tensor\n",
    "        Y = batch[1].to(device) # Ensure target labels are int64\n",
    "        #print(f\"Min Y: {Y.min()}, Max Y: {Y.max()}\")\n",
    "        optimizer.zero_grad()  \n",
    "        Y_hat = model_0(X)  # Model outputs logits\n",
    "        #print((Y.shape, Y_hat.shape))\n",
    "        loss = loss_fn(Y_hat, Y)  # Ensure correct shape for loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        Y_hat_classes = torch.argmax(Y_hat, dim=1)  # Get predicted class per pixel\n",
    "        # Compute accuracy\n",
    "        correct_pixels += (Y_hat_classes == Y).sum().item()\n",
    "        total_pixels += Y.numel()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches  # Compute mean loss for logging\n",
    "    loss_list.append(avg_loss)\n",
    "    accuracy = correct_pixels / total_pixels\n",
    "    print(f\"\\tEpoch {epoch} - Loss: {loss.item()}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
